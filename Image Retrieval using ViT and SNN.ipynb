{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1gERlAYEjVmKNrUGEQRirrhKG3ICOnZoi","authorship_tag":"ABX9TyN77JD+IxM6IpnVbgQTbgQ8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S0-pzowUKtpu","executionInfo":{"status":"ok","timestamp":1757047152419,"user_tz":-330,"elapsed":12345,"user":{"displayName":"Poorna Nithin R","userId":"05131205118169274840"}},"outputId":"0d89ba48-e722-40d2-a325-e867458ea952"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m122.9/125.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# =========================\n","# Cell 0: Install dependencies (run once)\n","# =========================\n","!pip install --quiet nibabel tqdm scikit-image opencv-python matplotlib scikit-learn torch torchvision snntorch timm"]},{"cell_type":"code","source":["# =========================\n","# Cell 1: Mount Google Drive (if needed)\n","# =========================\n","from google.colab import drive\n","drive.mount('/content/drive')   # follow prompts\n","\n","# Example structure expected (recommended):\n","# /content/drive/MyDrive/MM_NII/\n","#   ├── subj_0001/\n","#   │     ├── T1.nii.gz\n","#   │     ├── T2.nii.gz\n","#   │     └── FLAIR.nii.gz\n","#   ├── subj_0002/\n","#   │     ├── T1.nii.gz\n","#   │     ├── T2.nii.gz\n","#   │     └── FLAIR.nii.gz\n","#   └── ...\n","\n","# If your data is flat (all files in one folder), see the NOTE inside Cell 3 to adapt matching by filename substrings.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XgO0jzsZK84K","executionInfo":{"status":"ok","timestamp":1757047285275,"user_tz":-330,"elapsed":10247,"user":{"displayName":"Poorna Nithin R","userId":"05131205118169274840"}},"outputId":"addf7507-5559-4a8c-e603-862dea4edf04"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# =========================\n","# Cell 2: Imports + device + global settings\n","# =========================\n","import os, glob, math, random, time, re\n","from tqdm import tqdm\n","import numpy as np\n","import nibabel as nib\n","import matplotlib.pyplot as plt\n","import cv2\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as T\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Repro\n","random.seed(42); np.random.seed(42); torch.manual_seed(42)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Device:', device)\n","\n","# ============ EDIT ME ============\n","DATA_DIR = '/content/drive/MyDrive/MRI Extracted'   # root folder containing subject subfolders\n","MODALITIES = ['T1', 'T2', 'FLAIR']           # choose the modalities you want to fuse as channels\n","BASE_MODALITY_FOR_VIS = 'T1'                 # which modality to use for overlays/visuals\n","# =================================\n","\n","# Model/runtime sizes (shrink if OOM)\n","IMG_SIZE = 224\n","PATCH_SIZE = 16\n","EMBED_DIM = 128      # try 64 to save RAM\n","NUM_LAYERS = 4\n","NUM_HEADS = 8\n","BATCH_SIZE = 4       # reduce if OOM\n","TTFS_T = 30\n","SNN_OUT_NEURONS = 128\n","\n","assert IMG_SIZE % PATCH_SIZE == 0, \"IMG_SIZE must be divisible by PATCH_SIZE\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UyE-L7KTK816","executionInfo":{"status":"ok","timestamp":1757047304238,"user_tz":-330,"elapsed":15867,"user":{"displayName":"Poorna Nithin R","userId":"05131205118169274840"}},"outputId":"29de2e6f-fd39-4700-c405-76685a4728ca"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cpu\n"]}]},{"cell_type":"code","source":["# =========================\n","# Cell 3: Index subjects and their modality files\n","# =========================\n","def find_subjects_with_modalities(root, modalities):\n","    \"\"\"\n","    Expected: root/subj_xxx/<files containing modality names>\n","    We match files by the presence of modality keyword (case-insensitive) in filename.\n","    \"\"\"\n","    subjects = []\n","    subdirs = sorted([d for d in glob.glob(os.path.join(root, '*')) if os.path.isdir(d)])\n","    for sdir in subdirs:\n","        sid = os.path.basename(sdir)\n","        mod2file = {}\n","        for m in modalities:\n","            # match file that contains the modality token (case-insensitive)\n","            cand = sorted(glob.glob(os.path.join(sdir, f'*{m}*.nii*')))  # flexible: .nii or .nii.gz\n","            if len(cand) == 0:\n","                mod2file[m] = None\n","            else:\n","                mod2file[m] = cand[0]\n","        # keep subject only if all required modalities exist\n","        if all(mod2file[m] is not None for m in modalities):\n","            subjects.append((sid, mod2file))\n","    return subjects\n","\n","subjects = find_subjects_with_modalities(DATA_DIR, MODALITIES)\n","print(f\"Found {len(subjects)} subjects with modalities {MODALITIES}\")\n","\n","# NOTE (flat folder case):\n","# If your data is all in one folder, create a CSV mapping or use filename rules to group by subject_id.\n","# Example heuristic: subject id is the prefix before first '_' and modality appears in name.\n","# Then, build {subject_id: {mod: path}} and keep only subjects containing all required modalities.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72SwddrhK8zb","executionInfo":{"status":"ok","timestamp":1757047322867,"user_tz":-330,"elapsed":31,"user":{"displayName":"Poorna Nithin R","userId":"05131205118169274840"}},"outputId":"46dce5f1-6f1a-41db-907a-660badf8a5ee"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 0 subjects with modalities ['T1', 'T2', 'FLAIR']\n"]}]},{"cell_type":"code","source":["# =========================\n","# Cell 4: Utilities — load center slice aligned across modalities + preprocessing\n","# =========================\n","def robust_minmax(x, lo=1, hi=99):\n","    \"\"\"Percentile min-max -> [0,1]\"\"\"\n","    x = np.nan_to_num(x)\n","    lo_v, hi_v = np.percentile(x, [lo, hi])\n","    if hi_v - lo_v < 1e-8:\n","        return np.zeros_like(x, dtype=np.float32)\n","    x = np.clip(x, lo_v, hi_v)\n","    x = (x - lo_v) / (hi_v - lo_v + 1e-8)\n","    return x.astype(np.float32)\n","\n","def load_center_slice_volume(path):\n","    vol = nib.load(path).get_fdata()\n","    z = vol.shape[2] // 2\n","    sl = vol[:, :, z]\n","    return sl  # 2D slice\n","\n","def resize_2d(img2d, size=IMG_SIZE):\n","    return cv2.resize(img2d, (size, size), interpolation=cv2.INTER_CUBIC).astype(np.float32)\n","\n","def build_multimodal_center_slice(mod2file, modalities, ref_modal):\n","    \"\"\"\n","    Returns a stacked array shape (C, H, W) with C=len(modalities).\n","    Uses the z-center slice index from the reference modality for alignment.\n","    \"\"\"\n","    # reference z from ref modality\n","    ref_vol = nib.load(mod2file[ref_modal]).get_fdata()\n","    z_ref = ref_vol.shape[2] // 2\n","\n","    chans = []\n","    for m in modalities:\n","        vol = nib.load(mod2file[m]).get_fdata()\n","        z = min(z_ref, vol.shape[2]-1)  # clamp if depths differ\n","        sl = vol[:, :, z]\n","        sl = robust_minmax(sl)          # [0,1]\n","        sl = resize_2d(sl, IMG_SIZE)    # HxW\n","        chans.append(sl)\n","    mm = np.stack(chans, axis=0)        # CxHxW\n","    return mm\n","\n","# Quick sanity visualization on the first subject\n","if len(subjects) > 0:\n","    sid0, mfiles0 = subjects[0]\n","    mm0 = build_multimodal_center_slice(mfiles0, MODALITIES, BASE_MODALITY_FOR_VIS)\n","    print(\"Sample multi-modal tensor shape (C,H,W):\", mm0.shape)\n","    # show each modality\n","    n = mm0.shape[0]\n","    plt.figure(figsize=(4*n, 4))\n","    for i in range(n):\n","        plt.subplot(1, n, i+1)\n","        plt.imshow(mm0[i], cmap='gray')\n","        plt.title(f'{sid0} - {MODALITIES[i]} (center slice)'); plt.axis('off')\n","    plt.show()\n"],"metadata":{"id":"BeXYvT68K8xD","executionInfo":{"status":"ok","timestamp":1757047327644,"user_tz":-330,"elapsed":4,"user":{"displayName":"Poorna Nithin R","userId":"05131205118169274840"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Cell 5: Dataset for multimodal center slices\n","# =========================\n","class MultiModalCenterSliceDataset(Dataset):\n","    def __init__(self, subjects, modalities, ref_modal, transform=None):\n","        self.subjects = subjects\n","        self.modalities = modalities\n","        self.ref_modal = ref_modal\n","        self.transform = transform  # not used (we pre-resize), but kept for API symmetry\n","    def __len__(self):\n","        return len(self.subjects)\n","    def __getitem__(self, idx):\n","        sid, mod2file = self.subjects[idx]\n","        mm = build_multimodal_center_slice(mod2file, self.modalities, self.ref_modal)  # CxHxW in [0,1]\n","        # Normalize to [-1,1] per channel to match earlier code\n","        mm = (mm * 2.0 - 1.0).astype(np.float32)\n","        x = torch.from_numpy(mm)  # CxHxW\n","        return {'img': x, 'sid': sid, 'mod2file': mod2file}\n","\n","dataset = MultiModalCenterSliceDataset(subjects, MODALITIES, BASE_MODALITY_FOR_VIS)\n","print(\"Dataset size (subjects):\", len(dataset))\n","\n","# Visualize one subject stacked as RGB-like (if exactly 3 modalities)\n","if len(dataset) > 0:\n","    sample = dataset[0]\n","    x = sample['img'].numpy()  # CxHxW in [-1,1]\n","    # show channels individually (de-normalize to [0,1] for display)\n","    n = x.shape[0]\n","    plt.figure(figsize=(4*n, 4))\n","    for i in range(n):\n","        plt.subplot(1, n, i+1)\n","        plt.imshow((x[i]*0.5+0.5), cmap='gray'); plt.axis('off'); plt.title(MODALITIES[i])\n","    plt.suptitle(f\"Subject: {sample['sid']}  (multi-modal center slices)\")\n","    plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6PMRYovK8uq","executionInfo":{"status":"ok","timestamp":1757047330710,"user_tz":-330,"elapsed":16,"user":{"displayName":"Poorna Nithin R","userId":"05131205118169274840"}},"outputId":"14d30b3e-2211-4f26-9c97-7bc91e7951be"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset size (subjects): 0\n"]}]},{"cell_type":"code","source":["import os\n","\n","root = \"/content/drive/MyDrive/MRI Extracted\"   # change to your actual path\n","all_files = []\n","for path, dirs, files in os.walk(root):\n","    for f in files:\n","        if f.endswith(\".nii\") or f.endswith(\".nii.gz\"):\n","            all_files.append(os.path.join(path, f))\n","\n","print(\"Found files:\", len(all_files))\n","print(all_files[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kBGcBP0L56z","executionInfo":{"status":"ok","timestamp":1757047410109,"user_tz":-330,"elapsed":13,"user":{"displayName":"Poorna Nithin R","userId":"05131205118169274840"}},"outputId":"e0f79d92-bc48-4190-e195-bd7fd53897d2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Found files: 180\n","['/content/drive/MyDrive/MRI Extracted/patient6mri.nii', '/content/drive/MyDrive/MRI Extracted/patient8mri.nii', '/content/drive/MyDrive/MRI Extracted/patient2mri.nii', '/content/drive/MyDrive/MRI Extracted/patient4mri.nii', '/content/drive/MyDrive/MRI Extracted/patient5mri.nii', '/content/drive/MyDrive/MRI Extracted/patient1mri.nii', '/content/drive/MyDrive/MRI Extracted/patient7mri.nii', '/content/drive/MyDrive/MRI Extracted/patient3mri.nii', '/content/drive/MyDrive/MRI Extracted/patient26mri.nii', '/content/drive/MyDrive/MRI Extracted/patient12mri.nii']\n"]}]},{"cell_type":"code","source":["class MultiModalDataset(torch.utils.data.Dataset):\n","    def __init__(self, subjects, modalities, ref_modal):\n","        self.subjects = subjects\n","        self.modalities = modalities\n","        self.ref_modal = ref_modal\n","\n","    def __len__(self):\n","        return len(self.subjects)\n","\n","    def __getitem__(self, idx):\n","        if len(self.subjects) == 0:\n","            raise ValueError(\"❌ No subjects found! Check dataset path and preprocessing.\")\n","        sid, mod2file = self.subjects[idx]\n","        mm = build_multimodal_center_slice(mod2file, self.modalities, self.ref_modal)\n","        mm = (mm - 0.5) / 0.5\n","        return {'id': sid, 'img': mm}"],"metadata":{"id":"32_jr1VaMgtK","executionInfo":{"status":"ok","timestamp":1757047566726,"user_tz":-330,"elapsed":11,"user":{"displayName":"Poorna Nithin R","userId":"05131205118169274840"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Cell 6: Conv Tokenizer (multi-channel) + patch grid visualization\n","# =========================\n","class ConvTokenizer(nn.Module):\n","    def __init__(self, in_ch, embed_dim=EMBED_DIM, patch_size=PATCH_SIZE, img_size=IMG_SIZE):\n","        super().__init__()\n","        self.patch_size = patch_size\n","        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)\n","        n_patches = (img_size // patch_size) ** 2\n","        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n","        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n","        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n","        nn.init.trunc_normal_(self.cls_token, std=0.02)\n","    def forward(self, x):\n","        x = self.proj(x)                   # B x E x H' x W'\n","        B, C, H, W = x.shape\n","        x_flat = x.flatten(2).transpose(1,2)   # B x N x E\n","        cls = self.cls_token.expand(B, -1, -1)\n","        tokens = torch.cat([cls, x_flat], dim=1)\n","        tokens = tokens + self.pos_embed[:, :tokens.size(1), :]\n","        return tokens, x\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, embed_dim=EMBED_DIM, num_heads=NUM_HEADS, mlp_dim=EMBED_DIM*2, dropout=0.0):\n","        super().__init__()\n","        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(embed_dim, mlp_dim), nn.GELU(), nn.Dropout(dropout),\n","            nn.Linear(mlp_dim, embed_dim), nn.Dropout(dropout)\n","        )\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","    def forward(self, x):\n","        y, attn = self.attn(x, x, x, need_weights=True, average_attn_weights=False)\n","        x = self.norm1(x + y)\n","        x = self.norm2(x + self.mlp(x))\n","        return x, attn\n","\n","class SmallViT(nn.Module):\n","    def __init__(self, in_ch, embed_dim=EMBED_DIM, patch_size=PATCH_SIZE, img_size=IMG_SIZE, num_layers=NUM_LAYERS, num_heads=NUM_HEADS):\n","        super().__init__()\n","        self.tokenizer = ConvTokenizer(in_ch, embed_dim, patch_size, img_size)\n","        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads) for _ in range(num_layers)])\n","    def forward(self, x, return_attn=False):\n","        tokens, conv_feat = self.tokenizer(x)\n","        attn_maps = []\n","        for blk in self.blocks:\n","            tokens, attn = blk(tokens)\n","            attn_maps.append(attn)\n","        pooled = tokens[:, 1:, :].mean(1)\n","        if return_attn:\n","            return pooled, tokens, attn_maps, conv_feat\n","        return pooled, tokens, None, conv_feat\n","\n","model = SmallViT(in_ch=len(MODALITIES)).to(device)\n","model.eval()\n","\n","# one forward + visuals\n","item0 = dataset[0]\n","img0 = item0['img'].unsqueeze(0).to(device)   # 1 x C x H x W\n","with torch.no_grad():\n","    pooled0, tokens0, attn_maps0, conv_feat0 = model(img0, return_attn=True)\n","\n","print('tokens shape:', tokens0.shape, 'conv_feat shape:', conv_feat0.shape, 'pooled shape:', pooled0.shape)\n","\n","# visualize conv feature map (avg channels)\n","feat_map = conv_feat0[0].mean(0).detach().cpu().numpy()\n","plt.figure(figsize=(4,4)); plt.imshow(feat_map, cmap='inferno'); plt.title('Conv feature map (avg over embed channels)'); plt.axis('off'); plt.show()\n","\n","# overlay patch grid on BASE modality\n","base_idx = MODALITIES.index(BASE_MODALITY_FOR_VIS)\n","base_img_vis = (item0['img'][base_idx].numpy()*0.5+0.5)\n","plt.figure(figsize=(4,4)); plt.imshow(base_img_vis, cmap='gray')\n","p = IMG_SIZE // PATCH_SIZE\n","for i in range(1, p):\n","    plt.axvline(i*PATCH_SIZE, color='cyan', linewidth=0.6)\n","    plt.axhline(i*PATCH_SIZE, color='cyan', linewidth=0.6)\n","plt.title(f'Patch grid ({BASE_MODALITY_FOR_VIS})'); plt.axis('off'); plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"towqUpG6K8sK","executionInfo":{"status":"error","timestamp":1757047568642,"user_tz":-330,"elapsed":33,"user":{"displayName":"Poorna Nithin R","userId":"05131205118169274840"}},"outputId":"3517abd7-e62e-4772-802a-d45bdae7c90b"},"execution_count":16,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"list index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2339422922.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# one forward + visuals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mitem0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0mimg0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 1 x C x H x W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3614335642.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubjects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod2file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubjects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_multimodal_center_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod2file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodalities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_modal\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# CxHxW in [0,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Normalize to [-1,1] per channel to match earlier code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","source":["# =========================\n","# Cell 7: Attention visualization + rollout overlay (on base modality)\n","# =========================\n","def attention_rollout(attn_maps):\n","    \"\"\"\n","    attn_maps: list of (B, heads, S, S)\n","    returns: (S, S) rollout for batch 0\n","    \"\"\"\n","    result = None\n","    for att in attn_maps:\n","        a = att.mean(dim=1).detach().cpu().numpy()  # B x S x S\n","        a = a + np.eye(a.shape[-1])[None]\n","        a = a / (a.sum(axis=-1, keepdims=True) + 1e-8)\n","        result = a if result is None else np.matmul(result, a)\n","    return result[0]  # SxS\n","\n","roll0 = attention_rollout(attn_maps0)\n","cls_to_patch = roll0[0, 1:]\n","patch_count = cls_to_patch.shape[0]\n","pgrid = int(math.sqrt(patch_count))\n","if pgrid*pgrid == patch_count:\n","    heat = cls_to_patch.reshape(pgrid, pgrid)\n","    heat_resized = cv2.resize(heat, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_CUBIC)\n","\n","    plt.figure(figsize=(6,6))\n","    plt.imshow(base_img_vis, cmap='gray')\n","    plt.imshow(heat_resized, cmap='jet', alpha=0.45)\n","    plt.axis('off'); plt.title('Attention rollout overlay (base modality)')\n","    plt.show()\n","else:\n","    print(\"Warning: non-square patch grid; adjust IMG_SIZE/PATCH_SIZE.\")\n"],"metadata":{"id":"zyzc2xrzK8py"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Cell 8: TTFS encoding (pooled embedding -> spike raster) + visualization\n","# =========================\n","def ttfs_encode_vector(vec, T=TTFS_T):\n","    vec = vec.astype(np.float32)\n","    mn, mx = vec.min(), vec.max()\n","    norm = np.zeros_like(vec) if (mx-mn)<1e-8 else (vec-mn)/(mx-mn)\n","    spike_times = np.round((1.0 - norm) * (T-1)).astype(int)\n","    spike_train = np.zeros((T, vec.shape[0]), dtype=np.uint8)\n","    for i, t in enumerate(spike_times):\n","        spike_train[int(t), i] = 1\n","    return spike_train, spike_times\n","\n","pooled_np0 = pooled0[0].cpu().numpy()\n","spikes0, stimes0 = ttfs_encode_vector(pooled_np0)\n","\n","plt.figure(figsize=(8,5))\n","plt.imshow(spikes0.T, aspect='auto', cmap='gray_r')\n","plt.xlabel('Time'); plt.ylabel('Neuron index'); plt.title('TTFS raster (pooled embedding)')\n","plt.show()\n"],"metadata":{"id":"LKQz9YzSK8nT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LDZfoQSQK8kq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lK17pGrzK8gq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gU40QcuxK8eT"},"execution_count":null,"outputs":[]}]}